{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">SIT311 - SE3: Designing User-centric IoT Applications</span>\n",
    "\n",
    "\n",
    "## <span style=\"color:#0b486b\">Research implementation: machine learning techiniques for IoT security purpose</span>\n",
    "\n",
    "**Authors:**\n",
    "\n",
    "Duc Minh La - 217616227\n",
    "\n",
    "**Outline:** \n",
    "\n",
    "1. Set up and data preparation\n",
    "2. Methodology\n",
    "3. Results and discussion\n",
    "4. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">1. Set up and data preparation: </span>\n",
    "#### <span style=\"color:#0b486b\">1.1. Acknowledgement and introduction </span>\n",
    "This report is a continue from the previous research paper reagrding the machine learning solutions for IoT security. The research report have touched many different solutions for different security aspect such as authorization or malware dectection. This report will discuss and create an implementation of the malware detection solution using classification methods to distinguish between benign data and malicious data to detect viruses or malwares in the future.\n",
    "\n",
    "The used datases are progressed data from an open source[1] with the work of [2].\n",
    "\n",
    "#### <span style=\"color:#0b486b\">1.2. Set up the work place </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import the classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# import the checking method\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import the method for selecting best features\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">1.3. Data preparation </span>\n",
    "From the given source, there are basically 2 data file, 1 is benign data and other is the malacious data. To perform machine leaning classifications, the 2 dataset would be combined into 1 dataset with an extra feature or column named \"auth\" that show 1 for benign data and 0 for malacious data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*the shape of the malacious dataframe is: (59718, 115)\n",
      "*the shape of the benign dataframe is: (49548, 115)\n"
     ]
    }
   ],
   "source": [
    "#import the dataframe from csv file\n",
    "df1_o = pd.read_csv('combo.csv') # the malacious data\n",
    "df2_o = pd.read_csv('benign.csv') # the benign data\n",
    "#check the size of the dataset\n",
    "print('*the shape of the malacious dataframe is: {}'.format(df1_o.shape))\n",
    "print('*the shape of the benign dataframe is: {}'.format(df2_o.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the size of the 2 given dataset are extremly large, performing machine learning methods on the combined dataset would be extremely long [3]. This in real life scenario is understandable and accepted for the best result, plus with better computation resources such as cloud, the time can be reduced. However, for the sake of demonstration in this report and also because of the limitation of hardware, the tested would be reduced to only 2500 record from each dataset, hence, 5000 records to feed the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*the new shape of the malacious dataframe is: (2500, 115)\n",
      "*the new shape of the benign dataframe is: (2500, 115)\n"
     ]
    }
   ],
   "source": [
    "df1 = df1_o.head(n = 2500)\n",
    "df2 = df2_o.head(n = 2500)\n",
    "\n",
    "print('*the new shape of the malacious dataframe is: {}'.format(df1.shape))\n",
    "print('*the new shape of the benign dataframe is: {}'.format(df2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue, the dataset would go through data cleaning by dropping all the null values and then combined to create the dataset for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*the shape of the combined dataframe is: (5000, 115)\n"
     ]
    }
   ],
   "source": [
    "# dropping null values in both datasets - deletewise\n",
    "mala = df1.dropna()\n",
    "beni = df2.dropna() \n",
    "\n",
    "# combined 2 dataset to create the final dataset \n",
    "frames = [mala, beni]\n",
    "X = pd.concat(frames)\n",
    "print('*the shape of the combined dataframe is: {}'.format(X.shape))\n",
    "\n",
    "# getting the target array of auth\n",
    "mala['auth'] = 1\n",
    "beni['auth'] = 0\n",
    "frames2 = [mala, beni]\n",
    "combine = pd.concat(frames2)\n",
    "y = combine['auth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">2. Methodology: </span>\n",
    "As discussed above, the dataset is extremely large and for demonstration in this report, the dataset would be reduced. Though the number of records have been reduced, the number of features is still extremly high with the number of 115 features. The approach for reducing the number of features would be calculating the mutual information scores (FMI) for each features and choose the best features for machines learning. This is also a great method for improving the computating time of machine learning algorithms in real life, hence, different number of features would be used to test for the accuracy.\n",
    "#### <span style=\"color:#0b486b\">2.1. Calculating FMI scores </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>FMI</th>\n",
       "      <th>ranked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H_L0.01_weight</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MI_dir_L0.01_weight</td>\n",
       "      <td>0.690453</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HH_L0.01_magnitude</td>\n",
       "      <td>0.687814</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MI_dir_L0.01_mean</td>\n",
       "      <td>0.687397</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H_L0.01_mean</td>\n",
       "      <td>0.687397</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>HpHp_L3_pcc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>HpHp_L1_radius</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>HpHp_L1_covariance</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>HpHp_L1_pcc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>HpHp_L0.1_pcc</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                features       FMI  ranked\n",
       "0         H_L0.01_weight  0.690647     1.0\n",
       "1    MI_dir_L0.01_weight  0.690453     2.0\n",
       "2     HH_L0.01_magnitude  0.687814     3.0\n",
       "3      MI_dir_L0.01_mean  0.687397     4.5\n",
       "4           H_L0.01_mean  0.687397     4.5\n",
       "..                   ...       ...     ...\n",
       "110          HpHp_L3_pcc  0.000000   111.5\n",
       "111       HpHp_L1_radius  0.000000   111.5\n",
       "112   HpHp_L1_covariance  0.000000   111.5\n",
       "113          HpHp_L1_pcc  0.000000   111.5\n",
       "114        HpHp_L0.1_pcc  0.000000   111.5\n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = X.columns #get the feature names of later\n",
    "\n",
    "# calculating the FMI for each features in the d\n",
    "FMI = mutual_info_classif(X, y, random_state=20)\n",
    "# print(\"Features Mutual Info Scores: {}\".format(FMI))\n",
    "\n",
    "# connecting the calculated scores with the corresponding features\n",
    "di = {}\n",
    "for i, items in enumerate(FMI):\n",
    "    di[feature_names[i]] = items\n",
    "# print(di)\n",
    "\n",
    "# sorted the features\n",
    "sorted_d = sorted(di.items(), key=lambda x: x[1], reverse=True)\n",
    "# print(sorted_d)\n",
    "\n",
    "# putting the sorted features into an array\n",
    "ranked_ls = []\n",
    "for items in sorted_d:\n",
    "    ranked_ls.append(items[0])\n",
    "# print(ranked_ls)\n",
    "\n",
    "# create a table/dataframe for displaying the best features\n",
    "ranked_df = pd.DataFrame()\n",
    "ranked_df['features'] = ranked_ls\n",
    "ranked_df['FMI'] = sorted(FMI, reverse=True)\n",
    "ranked_df['ranked']=ranked_df['FMI'].rank(ascending=0)\n",
    "\n",
    "ranked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above, the report will test and comparing the results from 50, 25, 10 and 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the top 50 features are: \n",
      "['H_L0.01_weight', 'MI_dir_L0.01_weight', 'HH_L0.01_magnitude', 'MI_dir_L0.01_mean', 'H_L0.01_mean', 'HH_jit_L1_mean', 'H_L0.1_variance', 'MI_dir_L5_weight', 'H_L5_weight', 'H_L0.1_weight', 'H_L1_weight', 'MI_dir_L0.1_weight', 'MI_dir_L1_weight', 'H_L3_weight', 'MI_dir_L3_weight', 'H_L0.01_variance', 'MI_dir_L0.01_variance', 'HH_jit_L5_weight', 'HH_jit_L3_weight', 'MI_dir_L0.1_variance', 'HH_L5_weight', 'HH_jit_L0.1_mean', 'HH_L5_magnitude', 'HH_L3_weight', 'H_L1_mean', 'MI_dir_L1_mean', 'HH_L0.01_mean', 'HH_jit_L0.1_weight', 'HH_jit_L0.01_weight', 'HH_L0.1_weight', 'HH_L0.1_magnitude', 'HH_L1_magnitude', 'HH_L0.01_weight', 'HH_jit_L3_variance', 'H_L3_mean', 'MI_dir_L3_mean', 'HH_L1_weight', 'MI_dir_L5_mean', 'H_L5_mean', 'MI_dir_L0.1_mean', 'H_L0.1_mean', 'HH_jit_L1_weight', 'HH_jit_L5_variance', 'HH_jit_L1_variance', 'HH_L3_magnitude', 'HH_jit_L3_mean', 'HH_jit_L0.1_variance', 'HH_L0.1_mean', 'HH_L0.1_std', 'HH_L1_mean']\n",
      "\n",
      "the top 25 features are: \n",
      "['H_L0.01_weight', 'MI_dir_L0.01_weight', 'HH_L0.01_magnitude', 'MI_dir_L0.01_mean', 'H_L0.01_mean', 'HH_jit_L1_mean', 'H_L0.1_variance', 'MI_dir_L5_weight', 'H_L5_weight', 'H_L0.1_weight', 'H_L1_weight', 'MI_dir_L0.1_weight', 'MI_dir_L1_weight', 'H_L3_weight', 'MI_dir_L3_weight', 'H_L0.01_variance', 'MI_dir_L0.01_variance', 'HH_jit_L5_weight', 'HH_jit_L3_weight', 'MI_dir_L0.1_variance', 'HH_L5_weight', 'HH_jit_L0.1_mean', 'HH_L5_magnitude', 'HH_L3_weight', 'H_L1_mean']\n",
      "\n",
      "the top 10 features are: \n",
      "['H_L0.01_weight', 'MI_dir_L0.01_weight', 'HH_L0.01_magnitude', 'MI_dir_L0.01_mean', 'H_L0.01_mean', 'HH_jit_L1_mean', 'H_L0.1_variance', 'MI_dir_L5_weight', 'H_L5_weight', 'H_L0.1_weight']\n",
      "\n",
      "the top 5 features are: \n",
      "['H_L0.01_weight', 'MI_dir_L0.01_weight', 'HH_L0.01_magnitude', 'MI_dir_L0.01_mean', 'H_L0.01_mean']\n"
     ]
    }
   ],
   "source": [
    "print('the top 50 features are: ')\n",
    "print(list(ranked_df.head(n=50)['features']))\n",
    "print()\n",
    "print('the top 25 features are: ')\n",
    "print(list(ranked_df.head(n=25)['features']))\n",
    "print()\n",
    "print('the top 10 features are: ')\n",
    "print(list(ranked_df.head(n=10)['features']))\n",
    "print()\n",
    "print('the top 5 features are: ')\n",
    "print(list(ranked_df.head(n=5)['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">2.2. Using different machine learning techniques for classification </span>\n",
    "Extracting sub dataset for machine learning in each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_T50 = X[ranked_df.head(n=50)['features']] #extract data of top 50 only\n",
    "X_T25 = X[ranked_df.head(n=25)['features']] #extract data of top 25 only\n",
    "X_T10 = X[ranked_df.head(n=10)['features']] #extract data of top 10 only\n",
    "X_T5 = X[ranked_df.head(n=5)['features']] #extract data of top 5 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the lists to store values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_T50 = []\n",
    "ls_T25 = []\n",
    "ls_T10 = []\n",
    "ls_T5 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Apply KNeighborsClassifier() </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneig = KNeighborsClassifier()\n",
    "\n",
    "# Apply to top 50 only\n",
    "Scores50 = cross_val_score(kneig, X_T50, y, cv=10, scoring='accuracy')\n",
    "ls_T50.append(str(round(np.mean(Scores50) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 25 only\n",
    "Scores25 = cross_val_score(kneig, X_T25, y, cv=10, scoring='accuracy')\n",
    "ls_T25.append(str(round(np.mean(Scores25) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 10 only\n",
    "Scores10 = cross_val_score(kneig, X_T10, y, cv=10, scoring='accuracy')\n",
    "ls_T10.append(str(round(np.mean(Scores10) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 5 only\n",
    "Scores5 = cross_val_score(kneig, X_T5, y, cv=10, scoring='accuracy')\n",
    "ls_T5.append(str(round(np.mean(Scores5) * 100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Apply DecisionTreeClassifier() </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree =  DecisionTreeClassifier()\n",
    "\n",
    "#Apply to top 50 only\n",
    "Scores50 = cross_val_score(tree, X_T50, y, cv=10, scoring='accuracy')\n",
    "ls_T50.append(str(round(np.mean(Scores50) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 25 only\n",
    "Scores25 = cross_val_score(tree, X_T25, y, cv=10, scoring='accuracy')\n",
    "ls_T25.append(str(round(np.mean(Scores25) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 10 only\n",
    "Scores10 = cross_val_score(tree, X_T10, y, cv=10, scoring='accuracy')\n",
    "ls_T10.append(str(round(np.mean(Scores10) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 5 only\n",
    "Scores5 = cross_val_score(tree, X_T5, y, cv=10, scoring='accuracy')\n",
    "ls_T5.append(str(round(np.mean(Scores5) * 100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Apply GaussianNB() </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gau = GaussianNB()\n",
    "\n",
    "#Apply to top 50 only\n",
    "Scores50 = cross_val_score(gau, X_T50, y, cv=10, scoring='accuracy')\n",
    "ls_T50.append(str(round(np.mean(Scores50) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 25 only\n",
    "Scores25 = cross_val_score(gau, X_T25, y, cv=10, scoring='accuracy')\n",
    "ls_T25.append(str(round(np.mean(Scores25) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 10 only\n",
    "Scores10 = cross_val_score(gau, X_T10, y, cv=10, scoring='accuracy')\n",
    "ls_T10.append(str(round(np.mean(Scores10) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 5 only\n",
    "Scores5 = cross_val_score(gau, X_T5, y, cv=10, scoring='accuracy')\n",
    "ls_T5.append(str(round(np.mean(Scores5) * 100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Apply RandomForestClassifier(n_estimators=10) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "#Apply to top 50 only\n",
    "Scores50 = cross_val_score(forest, X_T50, y, cv=10, scoring='accuracy')\n",
    "ls_T50.append(str(round(np.mean(Scores50) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 25 only\n",
    "Scores25 = cross_val_score(forest, X_T25, y, cv=10, scoring='accuracy')\n",
    "ls_T25.append(str(round(np.mean(Scores25) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 10 only\n",
    "Scores10 = cross_val_score(forest, X_T10, y, cv=10, scoring='accuracy')\n",
    "ls_T10.append(str(round(np.mean(Scores10) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 5 only\n",
    "Scores5 = cross_val_score(forest, X_T5, y, cv=10, scoring='accuracy')\n",
    "ls_T5.append(str(round(np.mean(Scores5) * 100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\">Apply AdaBoostClassifier()  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier() \n",
    "\n",
    "#Apply to top 50 only\n",
    "Scores50 = cross_val_score(ada, X_T50, y, cv=10, scoring='accuracy')\n",
    "ls_T50.append(str(round(np.mean(Scores50) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 25 only\n",
    "Scores25 = cross_val_score(ada, X_T25, y, cv=10, scoring='accuracy')\n",
    "ls_T25.append(str(round(np.mean(Scores25) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 10 only\n",
    "Scores10 = cross_val_score(ada, X_T10, y, cv=10, scoring='accuracy')\n",
    "ls_T10.append(str(round(np.mean(Scores10) * 100,2)) + \"%\")\n",
    "\n",
    "#Apply to top 5 only\n",
    "Scores5 = cross_val_score(ada, X_T5, y, cv=10, scoring='accuracy')\n",
    "ls_T5.append(str(round(np.mean(Scores5) * 100,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">3. Results and discussion </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifiers</th>\n",
       "      <th>Accuracy top 50 features</th>\n",
       "      <th>Accuracy top 25 features</th>\n",
       "      <th>Accuracy top 10 features</th>\n",
       "      <th>Accuracy top 5 features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>98.92%</td>\n",
       "      <td>98.66%</td>\n",
       "      <td>99.3%</td>\n",
       "      <td>99.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>99.84%</td>\n",
       "      <td>99.82%</td>\n",
       "      <td>99.86%</td>\n",
       "      <td>99.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>53.92%</td>\n",
       "      <td>53.6%</td>\n",
       "      <td>54.28%</td>\n",
       "      <td>99.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>99.88%</td>\n",
       "      <td>99.86%</td>\n",
       "      <td>99.88%</td>\n",
       "      <td>99.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>99.84%</td>\n",
       "      <td>99.86%</td>\n",
       "      <td>99.88%</td>\n",
       "      <td>99.86%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Classifiers Accuracy top 50 features Accuracy top 25 features  \\\n",
       "0    KNeighbors                   98.92%                   98.66%   \n",
       "1  DecisionTree                   99.84%                   99.82%   \n",
       "2    GaussianNB                   53.92%                    53.6%   \n",
       "3  RandomForest                   99.88%                   99.86%   \n",
       "4      AdaBoost                   99.84%                   99.86%   \n",
       "\n",
       "  Accuracy top 10 features Accuracy top 5 features  \n",
       "0                    99.3%                  99.88%  \n",
       "1                   99.86%                  99.88%  \n",
       "2                   54.28%                  99.88%  \n",
       "3                   99.88%                  99.88%  \n",
       "4                   99.88%                  99.86%  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = ['KNeighbors', 'DecisionTree','GaussianNB', 'RandomForest', 'AdaBoost']\n",
    "\n",
    "df_ML = pd.DataFrame()\n",
    "df_ML['Classifiers'] = classifiers\n",
    "df_ML['Accuracy top 50 features'] = ls_T50\n",
    "df_ML['Accuracy top 25 features'] = ls_T25\n",
    "df_ML['Accuracy top 10 features'] = ls_T10\n",
    "df_ML['Accuracy top 5 features'] = ls_T5\n",
    "\n",
    "df_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, it is very interesting that all classifier perform almost the best when only using 5 features. This can be explained that more data can create more noises (noisy data) that provide false or unneeded insights that will create false results in the end. However, from all the classifier, KNeighbors, Decision Tree, RandomForest and AdaBoost have extremely high results for all the cases. This also because that the malacious data have an extreme different set of data that make the classifiers easier in detecting them. All in all, based on the final results, RandomForest has the best result that is consistent through out the cases while the GaussianNB has the worst results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4. References: </span>\n",
    "\n",
    "[1]: Y. Mirsky, T. Doitshman, Y. Elovici & A. Shabtai 2018, 'Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection', in Network and Distributed System Security (NDSS) Symposium, San Diego, CA, USA.\n",
    "\n",
    "[2]: Y. Meidan, M. Bohadana, Y. Mathov, Y. Mirsky, D. Breitenbacher, A. Shabtai, and Y. Elovici 'N-BaIoT: Network-based Detection of IoT Botnet Attacks Using Deep Autoencoders', IEEE Pervasive Computing, Special Issue - Securing the IoT (July/Sep 2018).\n",
    "\n",
    "[3]: Sze, Vivienne, et al. \"Hardware for machine learning: Challenges and opportunities.\" 2017 IEEE Custom Integrated Circuits Conference (CICC). IEEE, 2017."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
